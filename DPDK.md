[TOC]

# iavf

The iavf virtual function (VF) driver supports virtual functions generated by the physical function (PF) driver, with one or more VFs
enabled through sysfs. The associated PF drivers for this VF driver are:

* ice
* i40e

https://github.com/intel/ethernet-linux-iavf      简而言之: Intel 网卡的vf驱动.

> Transitioning from i40evf to iavf
> October 20, 2020
> Overview
> Intel created the Intel® Adaptive Virtual Function (iavf) driver to provide a consistent, future-proof virtual
> function (VF) interface for Intel® Ethernet controllers. Previously, when you upgraded your network hardware,
> you replaced the drivers in each virtual machine (VM) with new drivers that were capable of accessing the
> new VF device provided by the new hardware. The iavf driver allows you to upgrade your network hardware
> without the need to update the VF driver in your existing VMs.

# mempool

## create

```c
    /* create pktmbuf pool if it does not exist */
    pktmbuf_pool = rte_pktmbuf_pool_create("test_pktmbuf_pool",
            NB_MBUF, MEMPOOL_CACHE_SIZE, 0, MBUF_DATA_SIZE,
            SOCKET_ID_ANY);
```

## not preemptable

Note: the mempool implementation is not preemptable. A lcore must **not** be interrupted by another task that uses the same mempool (because it uses a ring which is not preemptable). 

Also, mempool functions must not be used outside the DPDK environment: for example, in linuxapp environment, a thread that is not created by the EAL must not use mempools. This is due to the per-lcore cache that won't work as rte_lcore_id() will not return a correct value.

所以 bbu pooling framework 中不能操作 mempool

有个说法是： create mempool 时可以指定per core 的cache size, 有说法是多个程序公用这个cache 可能会出问题， 这种情况不如将 cache size 设置为0.

`dpdk 在 pktmbuf_pool 的基础上添加了基于每个逻辑核的 mbuf cache 功能，在申请与释放的时候会优先使用 cache，避免直接操作 pktmbuf_pool 中的更底层的数据结构带来的性能损耗。`

`不过在复杂的使用场景中，我就遇到过数通引擎中多个线程绑定到同一个逻辑核中，并且共享了同一个 mempool 的情况。在这种场景中，mempool 中针对此逻辑核的 cache 被多个线程共享，当多个线程同时访问时就会出现不一致的情况，dpdk 内部并没有针对这个 cache 做互斥处理，常常遇到的情况是数通引擎莫名其妙段错误，查看位置发现与 mbuf 内容相关，但是看逻辑却解释不了 mbuf 的变化。`

`对于这种场景，可以针对性创建 cache_size 为 0 的 pktmbuf_pool 解决之。`

## 怪异之处

用一个ring 来存free mbuf, 所以一个mbuf 可以被重复free, 自然也就可以重复allocate.

# mbuf

## data structure

```c
struct rte_mbuf {
    void *buf_addr;           /**< Virtual address of segment buffer. */
    union {
        rte_iova_t buf_iova;
        rte_iova_t buf_physaddr; /**< deprecated */
    } __rte_aligned(sizeof(rte_iova_t));

    /* next 8 bytes are initialised on RX descriptor rearm */
    MARKER64 rearm_data;
    uint16_t data_off;//默认值就是 mbuf的headroom的大小

    /**
     * Reference counter. Its size should at least equal to the size
     * of port field (16 bits), to support zero-copy broadcast.
     * It should only be accessed using the following functions:
     * rte_mbuf_refcnt_update(), rte_mbuf_refcnt_read(), and
     * rte_mbuf_refcnt_set(). The functionality of these functions (atomic,
     * or non-atomic) is controlled by the CONFIG_RTE_MBUF_REFCNT_ATOMIC
     * config option.
     */
    RTE_STD_C11
    union {
        rte_atomic16_t refcnt_atomic; /**< Atomically accessed refcnt */
        /** Non-atomically accessed refcnt */
        uint16_t refcnt;
    };
    /** Input port (16 bits to support more than 256 virtual ports).
     * The event eth Tx adapter uses this field to specify the output port.
     */
    uint16_t port;

    uint64_t ol_flags;        /**< Offload features. */

    /* remaining bytes are set on RX when pulling packet from descriptor */
    MARKER rx_descriptor_fields1;

    /*
     * The packet type, which is the combination of outer/inner L2, L3, L4
     * and tunnel types. The packet_type is about data really present in the
     * mbuf. Example: if vlan stripping is enabled, a received vlan packet
     * would have RTE_PTYPE_L2_ETHER and not RTE_PTYPE_L2_VLAN because the
     * vlan is stripped from the data.
     */
    RTE_STD_C11
    union {
        uint32_t packet_type; /**< L2/L3/L4 and tunnel information. */
    };

    uint32_t pkt_len;         /**< Total pkt len: sum of all segments. */
    /*data区域一般指的是地址区间在 buf_addr + data_off 到 buf_add + data_off + data_len*/
    uint16_t data_len;        /**< Amount of data in segment buffer. */
    /** VLAN TCI (CPU order), valid if PKT_RX_VLAN is set. */
    uint16_t vlan_tci;

    RTE_STD_C11
    union {
            uint32_t rss;     /**< RSS hash result if RSS enabled */
    };
```

## 释放发送的mbuf

> For each packet to send, the rte_eth_tx_burst() function performs the following operations:
> 
> * Pick up the next available descriptor in the transmit ring.
> * Free the network buffer previously sent with that descriptor, if any.
> * Initialize the transmit descriptor with the information provided in the *rte_mbuf data structure.
> 
> In the case of a segmented packet composed of a list of rte_mbuf buffers, the rte_eth_tx_burst() function uses several transmit descriptors of the ring.

看来发送的时候完全可以用segmented packet 的mbuf

# 我的mbuf 优化建议

* 各个线程不要争用同一mempool, allocate/free mbuf 最好在一段代码中进行。
* headromm 不用128 byte 这么大。
* fronthual 上的normal IQ mubf size 根据 MTU 设置大小。 
* 与L1交互的buffer 不需要是mbuf.
* 改用  `send_message_burst()`  一次发送 32/64 根ant 的mbuf

# IOVA

目前 boot config 

```
intel_iommu=on iommu=pt 
```

In addition, to run the DPDK with Intel® VT-d, the iommu=pt kernel parameter must be used when using igb_uio driver. This results in pass-through of the DMAR (DMA Remapping) lookup in the host. Also, if INTEL_IOMMU_DEFAULT_ON is not set in the kernel, the intel_iommu=on kernel parameter must be used too. This ensures that the Intel IOMMU is being initialized as expected. 

Please note that while using iommu=pt is compulsory for igb_uio driver, the vfio-pci driver can actually work with both iommu=pt and iommu=on.

IO虚拟地址（IOVA）模式

DPDK是一个用户态应用框架，使用DPDK的软件可以像其他软件一样使用常规虚拟地址。但除此之外，DPDK还提供了用户态PMD和一组API，以实现完全以用户态执行IO操作。本系列的前一篇也已经提到过，**硬件不能使用用户空间虚拟地址;**它使用的是IO地址——物理地址（PA）或IO虚拟地址（IOVA）

**作为物理地址（PA）的IOVA模式**

作为PA的IOVA模式下，分配到整个DPDK存储区的IOVA地址都是实际的物理地址，而虚拟内存的分配与物理内存的分配相匹配。该模式的一大优点就是它很简单：它适用于所有硬件（也就是说，不需要IOMMU），并且它适用于内核空间（将真实物理地址转换为内核空间地址的开销是微不足道的）

作为PA的IOVA模式还有另外一个值得一提的限制——虚拟内存分配要遵循物理内存分配。这意味着如果物理内存空间被分段（被分成许多小段而不是几个大段）时，虚拟内存空间也要遵循同样的分段。极端情况下，分段可能过于严重，导致被分割出来物理上连续的片段数量过多，耗尽DPDK用于存储这些片段相关信息的内部数据结构，就会让DPDK初始化失败。

**作为虚拟地址（VA）的IOVA模式**

相比之下，作为VA的IOVA模式不需遵循底层物理内存的分布。而是重新分配物理内存，与虚拟内存的分配匹配。DPDK EAL依靠内核基础设施来实现这一点。内核基础设施又反过来使用IOMMU重新映射物理内存。

IOVA模式和DPDK PCI驱动程序

DPDK本身并不执行所有硬件设备寄存器和中断映射，它需要内核的帮助。为此，DPDK要使用的所有硬件设备都需要绑定到一个通用外围组件互连（Peripheral Component Interconnect, PCI）内核驱动程序。和一般的设备内核驱动程序不同的是，通用此驱动程序并未被锁定到特定的PCI ID集，即针对某类设备的常规驱动程序，可以与任何类型的PCI设备一起使用。

igb_uio驱动程序非常简单，能做的也并不多，因此它不支持使用IOMMU也就不足为奇了。或者，更确切地说，它确实支持IOMMU，但仅在PT模式下，它在IOVA和物理内存地址之间建立1：1映射。igb_uio不支持使用完整的IOMMU模式。因此， igb_uio驱动程序仅支持IOVA作为PA模式，并且根本无法在IOVA中作为VA模式工作。 

类似于igb_uio的驱动程序在内核中可用：uio_pci_generic。它的工作方式与igb_uio非常相似，只是它的功能更加有限。例如，igb_uio支持所有中断类型（传统，MSI和MSI） -X），而uio_pci_generic只支持遗留中断。更重要的是，igb_uio可以创建虚拟函数(Virtual Function, VF)，而uio_pci_generic则不能;因此，*如果在使用DPDK物理函数(Physical Function, PF)驱动程序时创建VF是必需的一步*，igb_uio是唯一的选择。  

因此，在大多数情况下，igb_uio与uio_pci_generic相同或更可取。关于使用IOMMU的所有限制同样适用于igb_uio和uio_pci_generic驱动程序 - 它们不能使用完整的IOMMU功能，因此仅支持IOVA作为PA模式。

**IOMMU**主要功能包括DMA Remapping和Interrupt Remapping，这里主要讲解DMA Remapping，Interrupt Remapping会独立讲解。对于DMA Remapping，IOMMU与MMU类似。IOMMU可以将一个设备访问地址转换为存储器地址，

对于有IOMMU的情况，网卡请求写入地址addr1会被IOMMU转换为addr2，然后发送到DDR控制器，最终访问的是DRAM上addr2地址，网卡访问的地址addr1会被IOMMU转换成真正的物理地址addr2，这里可以将addr1理解为虚机地址。

**alex: 为什么不能cpu 的MMU?**  我猜DPDK 网卡相当于一台CPU,不适合用其他的CPU的MMU, 所以必须要自己的MMU.

​           那一个mbuf, 其virtual address 在 MMU和IOMMU 前是一样的吗???

VFIO就是内核针对IOMMU提供的软件框架，支持DMA Remapping和Interrupt Remapping，这里只讲DMA Remapping。VFIO利用IOMMU这个特性，可以屏蔽物理地址对上层的可见性，可以用来开发用户态驱动，也可以实现设备透传。

# **rte_ring**

https://blog.csdn.net/qq_15437629/article/details/78147874

**rte_ring环形队列与基于链表的队列相比**，拥有如下优点：

- 速度更快，效率更高；

- rte_ring只需要一个CAS指令，而普通的链表队列则需要多个双重CAS指令

- 相对于普通无锁队列，实现更加简洁高效

- 支持批量入队和出队； 批量出队并不会像链氏队列一样，产生很多的**Cache Miss**, 此外批量出队和单独出队在花费上相差无几

**rte_ring环形队列与基于链表的队列相比**，缺点如下：

- rte_ring环形队列大小固定；
- rte_ring需要提前开辟空间，在未使用的情况下更容易造成内存的浪费；

每一个环形数据结构都包含两对（head，tail）指针：一对用于生产者（**prod**），另一队用于消费者（**cons**）。文章后面通过**prod_head**, **prod_tail**, **cons_head**, **cons_tail**来分别表示这四个指针 **head，tail的范围都是0~2^32；**，它恰恰是利用了unsigned int 溢出的性质。

在DPDK实现中，rte_ring是通过**“name”**字段来唯一标识的，我们可以通过`rte_ring_create()`来创建环形队列，他可以保证创建的队列name的唯一性。

> RING_F_SC_DEQ: If this flag is set, the default behavior when using rte_ring_dequeue() or rte_ring_dequeue_bulk() is "single-consumer". Otherwise, it is "multi-consumers".

## 单生产者/单消费者模式

入队操作: 只会修改生产者的head和tail指针（即prod_head, prod_tail）。在初始状态时，prod_head和prod_tail指向相同的内存空间。

* 将全局 {prod_head,cons.tail} 赋给 local variable  {prod_head,cons.tail}
* prod_next = prod_head + n( 入队数量)
* rte_smp_wmb()
* 将 local variable prod_next ---> 全局 {prod_tail}

# RTE Mbuf dynamic fields and flags

https://doc.dpdk.org/api/rte__mbuf__dyn_8h.html

用意： 利用 mbuf 的header, 
已经预定义了 tx/rx timestamp 的dyn field,  这个最好使用 hw timestamp

# digest

程序在退出前可调用 rte_eal_cleanup 释放内存等资源,不调用的话退出后会造成内存大页泄露
内存管理: 在 Linux 上, 支持在运行时获取/释放大页内存, 所以程序不需要在启动时获取内存

rte_mbuf 支持从外部内存分配

rte_graph 库: Graph 架构把数据处理函数抽象为 node, 并将其链接起来构成复杂的 graph. 这种架构使得数据处理高度可重用和模块化

支持注册非 EAL 线程

eCPRI 支持:  rte_flow 添加对 5G 网络的 eCPRI 报文的支持

基于时间戳的报文发送调度
